{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f392ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acec6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_token = \"3qGtNcgvCFOyype97IhPUIMpk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503768f5",
   "metadata": {},
   "source": [
    "# 1 Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62cc615",
   "metadata": {},
   "source": [
    "## 1.1 Downloading, cleaning, and filtering the 311 Service Request data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "100d9f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for: 2015-01-01 to 2015-01-31\n",
      "Downloading data for: 2015-02-01 to 2015-02-28\n",
      "Downloading data for: 2015-03-01 to 2015-03-31\n",
      "Downloading data for: 2015-04-01 to 2015-04-30\n",
      "Downloading data for: 2015-05-01 to 2015-05-31\n",
      "Downloading data for: 2015-06-01 to 2015-06-30\n",
      "Downloading data for: 2015-07-01 to 2015-07-31\n",
      "Downloading data for: 2015-08-01 to 2015-08-31\n",
      "Downloading data for: 2015-09-01 to 2015-09-30\n",
      "Downloading data for: 2015-10-01 to 2015-10-31\n",
      "Downloading data for: 2015-11-01 to 2015-11-30\n",
      "Downloading data for: 2015-12-01 to 2015-12-31\n",
      "Downloading data for: 2016-01-01 to 2016-01-31\n",
      "Downloading data for: 2016-02-01 to 2016-02-29\n",
      "Downloading data for: 2016-03-01 to 2016-03-31\n",
      "Downloading data for: 2016-04-01 to 2016-04-30\n",
      "Downloading data for: 2016-05-01 to 2016-05-31\n",
      "Downloading data for: 2016-06-01 to 2016-06-30\n",
      "Downloading data for: 2016-07-01 to 2016-07-31\n",
      "Downloading data for: 2016-08-01 to 2016-08-31\n",
      "Downloading data for: 2016-09-01 to 2016-09-30\n",
      "Downloading data for: 2016-10-01 to 2016-10-31\n",
      "Downloading data for: 2016-11-01 to 2016-11-30\n",
      "Downloading data for: 2016-12-01 to 2016-12-31\n",
      "Downloading data for: 2017-01-01 to 2017-01-31\n",
      "Downloading data for: 2017-02-01 to 2017-02-28\n",
      "Downloading data for: 2017-03-01 to 2017-03-31\n",
      "Downloading data for: 2017-04-01 to 2017-04-30\n",
      "Downloading data for: 2017-05-01 to 2017-05-31\n",
      "Downloading data for: 2017-06-01 to 2017-06-30\n",
      "Downloading data for: 2017-07-01 to 2017-07-31\n",
      "Downloading data for: 2017-08-01 to 2017-08-31\n",
      "Downloading data for: 2017-09-01 to 2017-09-30\n",
      "Downloading data for: 2017-10-01 to 2017-10-31\n",
      "Downloading data for: 2017-11-01 to 2017-11-30\n",
      "Downloading data for: 2017-12-01 to 2017-12-31\n",
      "Downloading data for: 2018-01-01 to 2018-01-31\n",
      "Downloading data for: 2018-02-01 to 2018-02-28\n",
      "Downloading data for: 2018-03-01 to 2018-03-31\n",
      "Downloading data for: 2018-04-01 to 2018-04-30\n",
      "Downloading data for: 2018-05-01 to 2018-05-31\n",
      "Downloading data for: 2018-06-01 to 2018-06-30\n",
      "Downloading data for: 2018-07-01 to 2018-07-31\n",
      "Downloading data for: 2018-08-01 to 2018-08-31\n",
      "Downloading data for: 2018-09-01 to 2018-09-30\n",
      "Downloading data for: 2018-10-01 to 2018-10-31\n",
      "Downloading data for: 2018-11-01 to 2018-11-30\n",
      "Downloading data for: 2018-12-01 to 2018-12-31\n",
      "Downloading data for: 2019-01-01 to 2019-01-31\n",
      "Downloading data for: 2019-02-01 to 2019-02-28\n",
      "Downloading data for: 2019-03-01 to 2019-03-31\n",
      "Downloading data for: 2019-04-01 to 2019-04-30\n",
      "Downloading data for: 2019-05-01 to 2019-05-31\n",
      "Downloading data for: 2019-06-01 to 2019-06-30\n",
      "Downloading data for: 2019-07-01 to 2019-07-31\n",
      "Downloading data for: 2019-08-01 to 2019-08-31\n",
      "Downloading data for: 2019-09-01 to 2019-09-30\n",
      "Downloading data for: 2019-10-01 to 2019-10-31\n",
      "Downloading data for: 2019-11-01 to 2019-11-30\n",
      "Downloading data for: 2019-12-01 to 2019-12-31\n",
      "Downloading data for: 2020-01-01 to 2020-01-31\n",
      "Downloading data for: 2020-02-01 to 2020-02-29\n",
      "Downloading data for: 2020-03-01 to 2020-03-31\n",
      "Downloading data for: 2020-04-01 to 2020-04-30\n",
      "Downloading data for: 2020-05-01 to 2020-05-31\n",
      "Downloading data for: 2020-06-01 to 2020-06-30\n",
      "Downloading data for: 2020-07-01 to 2020-07-31\n",
      "Downloading data for: 2020-08-01 to 2020-08-31\n",
      "Downloading data for: 2020-09-01 to 2020-09-30\n",
      "Downloading data for: 2020-10-01 to 2020-10-31\n",
      "Downloading data for: 2020-11-01 to 2020-11-30\n",
      "Downloading data for: 2020-12-01 to 2020-12-31\n",
      "Downloading data for: 2021-01-01 to 2021-01-31\n",
      "Downloading data for: 2021-02-01 to 2021-02-28\n",
      "Downloading data for: 2021-03-01 to 2021-03-31\n",
      "Downloading data for: 2021-04-01 to 2021-04-30\n",
      "Downloading data for: 2021-05-01 to 2021-05-31\n",
      "Downloading data for: 2021-06-01 to 2021-06-30\n",
      "Downloading data for: 2021-07-01 to 2021-07-31\n",
      "Downloading data for: 2021-08-01 to 2021-08-31\n",
      "Downloading data for: 2021-09-01 to 2021-09-30\n",
      "Downloading data for: 2021-10-01 to 2021-10-31\n",
      "Downloading data for: 2021-11-01 to 2021-11-30\n",
      "Downloading data for: 2021-12-01 to 2021-12-31\n",
      "Downloading data for: 2022-01-01 to 2022-01-31\n",
      "Downloading data for: 2022-02-01 to 2022-02-28\n",
      "Downloading data for: 2022-03-01 to 2022-03-31\n",
      "Downloading data for: 2022-04-01 to 2022-04-30\n",
      "Downloading data for: 2022-05-01 to 2022-05-31\n",
      "Downloading data for: 2022-06-01 to 2022-06-30\n",
      "Downloading data for: 2022-07-01 to 2022-07-31\n",
      "Downloading data for: 2022-08-01 to 2022-08-31\n",
      "Downloading data for: 2022-09-01 to 2022-09-30\n",
      "Downloading data for: 2022-10-01 to 2022-10-31\n",
      "Downloading data for: 2022-11-01 to 2022-11-30\n",
      "Downloading data for: 2022-12-01 to 2022-12-31\n",
      "Downloading data for: 2023-01-01 to 2023-01-31\n",
      "Downloading data for: 2023-02-01 to 2023-02-28\n",
      "Downloading data for: 2023-03-01 to 2023-03-31\n",
      "Downloading data for: 2023-04-01 to 2023-04-30\n",
      "Downloading data for: 2023-05-01 to 2023-05-31\n",
      "Downloading data for: 2023-06-01 to 2023-06-30\n",
      "Downloading data for: 2023-07-01 to 2023-07-31\n",
      "Downloading data for: 2023-08-01 to 2023-08-31\n",
      "Downloading data for: 2023-09-01 to 2023-09-30\n",
      "Cleaning data for the year: 2015\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'MANHATTAN'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/complaints_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaning data for the year: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(clean_convert(csv_path)\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mclean_convert\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_convert\u001b[39m(file_path):\n\u001b[0;32m     24\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborough\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m     df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m     26\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclosed_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6233\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m   6235\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m   6236\u001b[0m     ]\n\u001b[0;32m   6238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6239\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6240\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   6241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6243\u001b[0m \u001b[38;5;66;03m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:448\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastype\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, dtype, copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    524\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m--> 526\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    528\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    529\u001b[0m newb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;66;03m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    227\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     values \u001b[38;5;241m=\u001b[39m astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:170\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(arr\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'MANHATTAN'"
     ]
    }
   ],
   "source": [
    "api_token = \"3qGtNcgvCFOyype97IhPUIMpk\"\n",
    "\n",
    "def date_ranges(start_date, end_date):\n",
    "    return [(current, (current.replace(day=1) + timedelta(days=32)).replace(day=1) - timedelta(days=1))\n",
    "            for current in (start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1))\n",
    "            if current.day == 1]\n",
    "\n",
    "def fetch_data(start, end, token):\n",
    "    query = f\"created_date between '{start:%Y-%m-%dT%H:%M:%S}' and '{end:%Y-%m-%dT%H:%M:%S}'\"\n",
    "    response = requests.get(\"https://data.cityofnewyork.us/resource/erm2-nwe9.json\",\n",
    "                            params={\"$$app_token\": token, \"$where\": query, \"$limit\": 999999,\n",
    "                                    \"$select\": \"unique_key, created_date, closed_date, agency, complaint_type, descriptor, location_type, incident_zip, latitude, longitude, borough\"})\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "        return pd.DataFrame(response.json())\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error during API request: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_csv(df, file_path):\n",
    "    df.to_csv(file_path, mode='a', index=False, header=not os.path.exists(file_path))\n",
    "\n",
    "def clean_convert(file_path):\n",
    "    df = pd.read_csv(file_path, low_memory=False).dropna(subset=['borough'], how='any')\n",
    "    df[['latitude', 'longitude']] = df[['latitude', 'longitude']].astype(float)\n",
    "    df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "    df['closed_date'] = pd.to_datetime(df['closed_date'])\n",
    "    df['geometry'] = df.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry='geometry').set_crs(epsg=4326)\n",
    "    return gdf\n",
    "\n",
    "start_date, end_date = datetime(2015, 1, 1), datetime(2023, 9, 30)\n",
    "\n",
    "for start, end in date_ranges(start_date, end_date):\n",
    "    csv_path = f\"data/complaints_data_{start.year}.csv\"\n",
    "    print(f\"Downloading data for: {start:%Y-%m-%d} to {end:%Y-%m-%d}\")\n",
    "    process_csv(fetch_data(start, end, api_token), csv_path)\n",
    "\n",
    "for year in range(start_date.year, end_date.year + 1):\n",
    "    csv_path = f\"data/complaints_data_{year}.csv\"\n",
    "    print(f\"Cleaning data for the year: {year}\")\n",
    "    print(clean_convert(csv_path).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611d1215",
   "metadata": {},
   "source": [
    "## 1.2 Downloading, cleaning, and filtering the 2015 Tree Cencus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb4056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tree_id                           spc_latin health status zipcode  \\\n",
      "0       180683                         Acer rubrum   Fair  Alive   11375   \n",
      "1       200540                   Quercus palustris   Fair  Alive   11357   \n",
      "2       204026  Gleditsia triacanthos var. inermis   Good  Alive   11211   \n",
      "3       204337  Gleditsia triacanthos var. inermis   Good  Alive   11211   \n",
      "4       189565                     Tilia americana   Good  Alive   11215   \n",
      "...        ...                                 ...    ...    ...     ...   \n",
      "683783  155433                   Quercus palustris   Good  Alive   11211   \n",
      "683784  183795                 Cladrastis kentukea   Good  Alive   11375   \n",
      "683785  166161                         Acer rubrum   Good  Alive   10314   \n",
      "683786  184028                         Acer rubrum   Good  Alive   10457   \n",
      "683787  200607                         Acer rubrum   Fair  Alive   11365   \n",
      "\n",
      "           Latitude     longitude           spc_common  \\\n",
      "0       40.72309177  -73.84421522            red maple   \n",
      "1       40.79411067  -73.81867946              pin oak   \n",
      "2       40.71758074   -73.9366077          honeylocust   \n",
      "3       40.71353749  -73.93445616          honeylocust   \n",
      "4       40.66677776  -73.97597938      American linden   \n",
      "...             ...           ...                  ...   \n",
      "683783  40.71321078  -73.95494401              pin oak   \n",
      "683784  40.71519444   -73.8566502  Kentucky yellowwood   \n",
      "683785  40.62076153  -74.13651724            red maple   \n",
      "683786  40.85082819  -73.90311472            red maple   \n",
      "683787  40.73216525  -73.78752646            red maple   \n",
      "\n",
      "                          geometry  \n",
      "0       POINT (-73.84422 40.72309)  \n",
      "1       POINT (-73.81868 40.79411)  \n",
      "2       POINT (-73.93661 40.71758)  \n",
      "3       POINT (-73.93446 40.71354)  \n",
      "4       POINT (-73.97598 40.66678)  \n",
      "...                            ...  \n",
      "683783  POINT (-73.95494 40.71321)  \n",
      "683784  POINT (-73.85665 40.71519)  \n",
      "683785  POINT (-74.13652 40.62076)  \n",
      "683786  POINT (-73.90311 40.85083)  \n",
      "683787  POINT (-73.78753 40.73217)  \n",
      "\n",
      "[683788 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "json_url = \"https://data.cityofnewyork.us/api/views/5rq2-4hqu/rows.json?accessType=DOWNLOAD\"\n",
    "app_token = \"3qGtNcgvCFOyype97IhPUIMpk\"\n",
    "response = requests.get(json_url, params={\"$$app_token\": app_token})\n",
    "data = response.json()[\"data\"]\n",
    "column_names = [col[\"name\"] for col in response.json()[\"meta\"][\"view\"][\"columns\"]]\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "selected_columns = ['tree_id', 'spc_latin', 'health', 'status', 'zipcode', 'Latitude', 'longitude', 'spc_common']\n",
    "Tree_Cencus = df[selected_columns]\n",
    "Tree_Cencus = gpd.GeoDataFrame(Tree_Cencus, geometry=gpd.points_from_xy(Tree_Cencus.longitude, Tree_Cencus.Latitude), crs=\"EPSG:4326\")\n",
    "Tree_Cencus.crs = \"EPSG:4326\"\n",
    "print(Tree_Cencus)\n",
    "Tree_Cencus.to_csv('data/tree_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8bbc7",
   "metadata": {},
   "source": [
    "## 1.3 Cleaning and filtering the Zip Codes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f451571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_zipcode_data(file):\n",
    "    gdf = gpd.read_file(file)\n",
    "    gdf = gdf[['ZIPCODE', 'POPULATION', 'geometry' ]]\n",
    "    gdf.rename(columns={'ZIPCODE':'zipcode', 'POPULATION':'population', 'geometry':'geometry'}, inplace = True)\n",
    "    gdf = gdf.reset_index(drop = True)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966634af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  zipcode  population                                           geometry\n",
      "0   11436     18681.0  POLYGON ((1038098.252 188138.380, 1038141.936 ...\n",
      "1   11213     62426.0  POLYGON ((1001613.713 186926.440, 1002314.243 ...\n",
      "2   11212     83866.0  POLYGON ((1011174.276 183696.338, 1011373.584 ...\n",
      "3   11225     56527.0  POLYGON ((995908.365 183617.613, 996522.848 18...\n",
      "4   11218     72280.0  POLYGON ((991997.113 176307.496, 992042.798 17...\n",
      "    zipcode  population                                           geometry\n",
      "258   10310     25003.0  POLYGON ((950767.507 172848.969, 950787.510 17...\n",
      "259   11693     11052.0  POLYGON ((1028453.995 167153.410, 1027813.010 ...\n",
      "260   11249     28481.0  POLYGON ((995877.318 203206.075, 995968.511 20...\n",
      "261   10162         0.0  POLYGON ((997731.761 219560.922, 997641.948 21...\n",
      "262   10119         0.0  POLYGON ((986038.661 213051.063, 986135.314 21...\n"
     ]
    }
   ],
   "source": [
    "zipcode = clean_zipcode_data(r\"C:\\Users\\Victo\\Desktop\\nyc_zipcodes.shp\")\n",
    "print(zipcode.head())\n",
    "print(zipcode.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f756abd",
   "metadata": {},
   "source": [
    "## 1.4 Cleaning and filtering the Zillow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9abc3c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      RegionName       CountyName        date        value\n",
      "0          11385    Queens County  2018-10-31  2419.898399\n",
      "3          10467     Bronx County  2018-10-31  1774.365406\n",
      "5          11226     Kings County  2018-10-31  2366.890927\n",
      "6          11207     Kings County  2018-10-31  2249.035187\n",
      "7          10025  New York County  2018-10-31  3263.088691\n",
      "...          ...              ...         ...          ...\n",
      "8695       10282  New York County  2023-09-30  7347.458333\n",
      "8696       11109    Queens County  2023-09-30  4529.358974\n",
      "8697       10006  New York County  2023-09-30  4060.096154\n",
      "8698       10162  New York County  2023-09-30  5011.666667\n",
      "8699       10004  New York County  2023-09-30  4355.328283\n",
      "\n",
      "[5969 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "zillow = pd.read_csv(r'C:\\Users\\Victo\\Desktop\\zillow_rent_data.csv')\n",
    "zillow = zillow[zillow['City'] == 'New York']\n",
    "selected_columns = ['RegionName', 'CountyName']  # Removed extra square brackets\n",
    "region_and_county = zillow[selected_columns]  # Removed extra square brackets\n",
    "time_index = zillow.loc[:, '2018-09-30':]\n",
    "time_index = time_index.drop(time_index.columns[0], axis=1)\n",
    "zillow = pd.concat([region_and_county, time_index], axis=1)\n",
    "zillow = zillow.melt(id_vars=[\"RegionName\", \"CountyName\"], var_name=\"date\")\n",
    "zillow = zillow.reset_index(drop=True)\n",
    "zillow = zillow.dropna()\n",
    "print(zillow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48837b47",
   "metadata": {},
   "source": [
    "# 2 Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306984dd",
   "metadata": {},
   "source": [
    "## 2.1 Creating Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e916279d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!createdb LRJY_FINAL_PROJECT\n",
    "#This is where Lingwen and Joshua both are not able to run\n",
    "#Lingwen even attempted on his old PC, still not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff87ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql --dbname LRJY_FINAL_PROJECT -c 'CREATE EXTENSION postgis;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql --dbname LRJY_FINAL_PROJECT -f schema.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ace092",
   "metadata": {},
   "source": [
    "## 2.2 Creating Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_cencus_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS tree_census (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        geometry geometry,(Geometry, 4326)\n",
    "        health VARCHAR,\n",
    "        status VARCHAR,\n",
    "        zipcode INTEGER NOT NULL,\n",
    "        species VARCHAR,\n",
    "        latitude FLOAT,\n",
    "        longitude FLOAT\n",
    "        borough VARCHAR,\n",
    "    );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zip_codes_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS nyc_zip_codes (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        zipcode INTEGER NOT NULL,\n",
    "        population INTEGER,\n",
    "        geometry geometry(Geometry, 4326)\n",
    "    );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS zillow_rent (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        zipcode INTEGER NOT NULL,\n",
    "        county VARCHAR,\n",
    "        date DATE,\n",
    "        value FLOAT\n",
    "    );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(tree_cencus_table)\n",
    "cur.execute(nyc_zip_codes_table)\n",
    "cur.execute(zillow_table)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7dbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"dbname=LRJY_FINAL_PROJECT user=postgres password=postgres\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "for year in range(2015, 2024):\n",
    "    table_name = f\"=311_Service_Request_{year}\"\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            start_date DATE,\n",
    "            end_date DATE,\n",
    "            agency VARCHAR,\n",
    "            complaint_type VARCHAR,\n",
    "            descriptor VARCHAR,\n",
    "            location_type VARCHAR,\n",
    "            incident_zip INTEGER,\n",
    "            latitude FLOAT,\n",
    "            longitude FLOAT,\n",
    "            borough VARCHAR,\n",
    "            geometry geometry(Geometry, 4326)\n",
    "        );\n",
    "    \"\"\"\n",
    "    cur.execute(create_table_query)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88478507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac502214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034f12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7a78a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fbb10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb21d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bea04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe790c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8323b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ba7acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
